# Predicting firm trading behavior under a cap-and-trade program

## Files

I submit two EMR notebooks, named “part1_data.ipynb” and “part2_analysis.ipynb” respectively. 

## Introduction

Cap-and-trade is a type of market-based pollution control policy. Evaluation of cap-and-trade policies has been a hot research topic in the environmental policy literature. Under a cap-and-trade program, the government specifies the quantity of emission allowances to be distributed to emitting firms in a given time period (usually a year). After the initial allocation of allowances, participating firms can trade their emission allowances with each other. Economists have long been promoting market-based emissions control policies as a more cost-effective approach as a trading system offers more flexibility in the specific ways for affected sources to reduce emissions as long as the total emissions are under the cap.

In this project, I will analyze a particular cap-and-trade program called “Clean Air Interstate Rule (CAIR)”, which targets on reducing NOx emissions from the power sector in the east US starting from 2009. As firms covered by the program can make their own choices to buy, sell, or not trade their emission allowances, it is important to understand what factors can affect a firm’s trading decisions. Therefore, the major research problem I will analyze is: What factors can make a firm more likely to purchase allowances under CAIR? I believe that a better understanding of firm behaviors under such environmental regulation will help policymakers to improve the efficiency and effectiveness of policy design in the future. To solve the research problem, I will apply machine learning algorithms to predict firm trading behaviors using a firm-year level panel dataset.

## Data Description

The major data source I will use is a firm-year level dataset that I have assembled before. This dataset is named “emissions.csv” and stored in my s3 bucket named “noaadata”. The dataset includes 1483 firms in total. As the program started in 2009 and ended in 2014. I use data from 2003 to 2014 so that the dataset includes 6 years of pre-period and 6 years of post-period. Here I will briefly describe the type of variables included in the dataset by their data sources: (1) The major data source is the EPA air market databases. I used emissions and allowance transaction records from firms covered by the CAIR program as long as firms not covered by the CAIR program as control units. The variables include NOx emissions level, firm attributes (location, operating time, emitting unit type, fuel type, control technology type, etc.), and trading variables (buyer and seller status). (2) I also collected county-level demographic characteristics from the US Census. The variables include population density, urbanization ratio, median income, race, gender, age, etc.

As previous literature in environmental policy indicates, weather conditions are important factors that can affect emissions level. Therefore, I believe weather conditions are useful variables to add in my dataset. Initially, I downloaded site-based weather conditions data from NOAA climate data center and processed these data in an EMR notebook (part1_data.ipynb). However, as indicated in the part 1 notebook, further investigation reveals that the data quality is too bad, so I have to change the data source. Even though I will not use these data in the later analysis, I still think this is a good practice of large-computing techniques. Therefore, I still keep this notebook, but please notice that I do not use the data processed in this notebook in part 2 analysis. 

The data source I actually use for weather conditions is grid-cell based data from PRISM (https://prism.oregonstate.edu/explorer/). As firms’ longitude and latitude information are available from my original dataset, I download 4km resolution data using firm location information. These weather data files stored in the same s3 bucket are named as “weather1”, “weather2” and “weather3”. As these weather data are already at firm-year level, I only need to merge them with my main dataset. After merging the data, I also performed some data visualization tasks. For more details and explanations, please refer to the part 2 notebook.

## Machine Learning Models

As “buyer” is a binary variable, I will use a logistic regression as well as tree methods including decision tree, gradient-boosted tree and random forest. For logistic regression, I use 5-fold cross validation and different parameter values. The logistic regression model achieves a fairly good performance - an AUC of 0.78. I will also discuss about coefficients of some important features. As the coefficients for NOx and operating time are positive, this implies that firms with higher emissions level and firms with higher production level are more likely to become buyers. Most of the coefficients for NOx control technologies are negative, meaning that firms relying on technological advancements are less likely to become buyers in the trading market.

For tree-based methods, I also use 5-fold cross validation. The decision tree achieves an accuracy of 0.67. The gradient-boosted tree method achieves an accuracy of 0.82 and random forest achieves an accuracy of 0.72. Among the three methods, ensemble methods perform much better than the simple decision tree, and gradient-boosted tree has the best performance.

To conclude, the selected features including firm-level attributes, demographic characteristics and weather conditions can well predict a firm’s decision of whether to purchase allowances. 

## Discussion of PySpark experience

As PySpark is able to parallelize computation tasks automatically, it can be extremely helpful when your data size is large.  Another case where using PySpark for data analysis is useful is when you need to conduct complicated computation processes such as grid search. However, throughout the process of running this project, I also found some deficits of PySpark. The first deficitis that the EMR notebook sometimes lost connection to the PySpark kernel, then all objects saved before will be lost and I have to rerun the code, which can be very inefficient and resource-wasting. Another disadvantage is that some tasks can be more challenging, as many standard python packages are not available for PySpark. For example, you can easily visualize a decision tree model in sklearn, but there is no easy way to do the same thing in PySpark, so there is definitely a steep learning curve. Based on these thoughts, we should flexibly choose different computing tools based on specific research need in our future research life.
